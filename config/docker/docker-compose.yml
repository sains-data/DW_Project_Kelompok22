version: '3.8'

services:
  # SQL Server 2022 for Data Warehouse
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: ptxyz_sqlserver
    hostname: sqlserver
    environment:
      - ACCEPT_EULA=Y
      - SA_PASSWORD=${MSSQL_SA_PASSWORD}
      - MSSQL_PID=${MSSQL_PID}
    ports:
      - "1433:1433"
    volumes:
      - sqlserver_data:/var/opt/mssql
      - ./data/raw/Dataset:/data
      - ./misi3:/scripts
    networks:
      - dw_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "/opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P '${MSSQL_SA_PASSWORD}' -Q 'SELECT 1' -C -N"]
      interval: 30s
      timeout: 10s
      retries: 5

  # PostgreSQL for Airflow metadata
  postgres:
    image: postgres:13
    container_name: ptxyz_postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - dw_network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 5s
      retries: 5

  # Redis for Airflow Celery
  redis:
    image: redis:7-alpine
    container_name: ptxyz_redis
    command: redis-server --requirepass ${REDIS_PASSWORD}
    networks:
      - dw_network
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50

  # Airflow Webserver
  airflow-webserver:
    build: ./airflow
    container_name: ptxyz_airflow_webserver
    command: webserver
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__API__AUTH_BACKENDS=${AIRFLOW__API__AUTH_BACKENDS}
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME}
      - _AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      sqlserver:
        condition: service_healthy
    networks:
      - dw_network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Airflow Scheduler
  airflow-scheduler:
    build: ./airflow
    container_name: ptxyz_airflow_scheduler
    command: scheduler
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__API__AUTH_BACKENDS=${AIRFLOW__API__AUTH_BACKENDS}
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      sqlserver:
        condition: service_healthy
    networks:
      - dw_network
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5

  # Airflow Worker (for Celery Executor)
  airflow-worker:
    build: ./airflow
    container_name: ptxyz_airflow_worker
    command: celery worker
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID}
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__API__AUTH_BACKENDS=${AIRFLOW__API__AUTH_BACKENDS}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      sqlserver:
        condition: service_healthy
    networks:
      - dw_network
    healthcheck:
      test: ["CMD-SHELL", 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5

  # Apache Superset for Data Visualization
  superset:
    image: apache/superset:latest
    container_name: ptxyz_superset
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_SECRET_KEY=${SUPERSET_SECRET_KEY}
    volumes:
      - superset_data:/app/superset_home
      - ./configs/superset:/app/superset_config
    networks:
      - dw_network
    depends_on:
      sqlserver:
        condition: service_healthy
    command: >
      bash -c "
      superset db upgrade &&
      superset fab create-admin --username ${SUPERSET_USERNAME} --firstname Admin --lastname User --email ${SUPERSET_EMAIL} --password ${SUPERSET_PASSWORD} &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger
      "

  # Jupyter Notebook for Data Analysis
  jupyter:
    image: jupyter/datascience-notebook:latest
    container_name: ptxyz_jupyter
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=${JUPYTER_ENABLE_LAB}
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/home/jovyan/work/data
    networks:
      - dw_network
    command: start-notebook.sh --NotebookApp.token='${JUPYTER_TOKEN}' --NotebookApp.password=''

  # Grafana for Dashboard and Monitoring
  grafana:
    image: grafana/grafana:latest
    container_name: ptxyz_grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=${GF_USERS_ALLOW_SIGN_UP}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - dw_network
    depends_on:
      sqlserver:
        condition: service_healthy

  # Metabase as alternative BI tool
  metabase:
    image: metabase/metabase:latest
    container_name: ptxyz_metabase
    ports:
      - "3001:3000"
    environment:
      - MB_DB_TYPE=${MB_DB_TYPE}
      - MB_DB_DBNAME=${MB_DB_DBNAME}
      - MB_DB_PORT=${MB_DB_PORT}
      - MB_DB_USER=${MB_DB_USER}
      - MB_DB_PASS=${MB_DB_PASS}
      - MB_DB_HOST=${MB_DB_HOST}
    volumes:
      - metabase_data:/metabase-data
    networks:
      - dw_network
    depends_on:
      postgres_metabase:
        condition: service_healthy

  # PostgreSQL for Metabase
  postgres_metabase:
    image: postgres:13
    container_name: ptxyz_postgres_metabase
    environment:
      - POSTGRES_DB=metabase
      - POSTGRES_USER=metabase
      - POSTGRES_PASSWORD=metabase
    volumes:
      - postgres_metabase_data:/var/lib/postgresql/data
    networks:
      - dw_network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "metabase"]
      interval: 5s
      retries: 5

  # Database Initialization Service
  db_init:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: ptxyz_db_init
    environment:
      - SA_PASSWORD=${MSSQL_SA_PASSWORD}
    volumes:
      - ./init-scripts:/init-scripts
      - ./misi3:/scripts
    networks:
      - dw_network
    depends_on:
      sqlserver:
        condition: service_healthy
    command: >
      bash -c "
        echo 'ğŸš€ Starting database initialization...'
        
        # Wait for SQL Server to be fully ready
        echo 'â³ Waiting for SQL Server...'
        sleep 10
        
        # Test connection
        until /opt/mssql-tools18/bin/sqlcmd -S sqlserver -U sa -P '${MSSQL_SA_PASSWORD}' -Q 'SELECT 1' -C -N; do
          echo 'Waiting for SQL Server to be ready...'
          sleep 5
        done
        
        echo 'âœ… SQL Server is ready!'
        
        # Create database
        echo 'ğŸ“Š Creating PTXYZ_DataWarehouse database...'
        /opt/mssql-tools18/bin/sqlcmd -S sqlserver -U sa -P '${MSSQL_SA_PASSWORD}' -C -N -Q \"
        IF NOT EXISTS (SELECT name FROM sys.databases WHERE name = 'PTXYZ_DataWarehouse')
        BEGIN
            CREATE DATABASE PTXYZ_DataWarehouse;
            PRINT 'Database PTXYZ_DataWarehouse created successfully';
        END
        ELSE
        BEGIN
            PRINT 'Database PTXYZ_DataWarehouse already exists';
        END
        \"
        
        # Create schema
        echo 'ğŸ—ï¸  Creating database schema...'
        /opt/mssql-tools18/bin/sqlcmd -S sqlserver -U sa -P '${MSSQL_SA_PASSWORD}' -d PTXYZ_DataWarehouse -C -N -i /init-scripts/create-schema.sql
        
        echo 'ğŸ‰ Database initialization completed!'
        echo 'ğŸ“ˆ Ready for ETL operations!'
        
        # Keep container running for a bit to show logs
        sleep 30
      "
    restart: "no"

volumes:
  sqlserver_data:
  postgres_data:
  postgres_metabase_data:
  superset_data:
  grafana_data:
  metabase_data:

networks:
  dw_network:
    driver: bridge
