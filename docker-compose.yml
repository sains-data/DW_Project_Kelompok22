services:
  # SQL Server 2022 for Data Warehouse
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: ptxyz_sqlserver
    hostname: sqlserver
    environment:
      - ACCEPT_EULA=Y
      - SA_PASSWORD=${MSSQL_SA_PASSWORD:-PTXYZSecure123!}
      - MSSQL_SA_PASSWORD=${MSSQL_SA_PASSWORD:-PTXYZSecure123!}
      - MSSQL_PID=${MSSQL_PID:-Express}
      - MSSQL_USER=sa
    ports:
      - "1433:1433"
    volumes:
      - sqlserver_data:/var/opt/mssql
      - ./data:/data
      - ./init-scripts:/init-scripts
      - ./misi3:/scripts
    networks:
      - dw_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "/opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P 'PTXYZSecure123!' -Q 'SELECT 1' -C -N"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s

  # PostgreSQL for Airflow metadata
  postgres:
    image: postgres:13
    container_name: ptxyz_postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-airflow}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-airflow}
      - POSTGRES_DB=${POSTGRES_DB:-airflow}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - dw_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER:-airflow}"]
      interval: 5s
      retries: 5
      start_period: 30s

  # Redis for Airflow Celery
  redis:
    image: redis:7-alpine
    container_name: ptxyz_redis
    command: redis-server --requirepass ${REDIS_PASSWORD:-ptxyz123}
    networks:
      - dw_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-ptxyz123}", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
      start_period: 30s

  # Airflow Webserver
  airflow-webserver:
    build: 
      context: ./airflow
      dockerfile: Dockerfile
    container_name: ptxyz_airflow_webserver
    command: webserver
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID:-50000}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR:-CeleryExecutor}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:-postgresql+psycopg2://airflow:airflow@postgres/airflow}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD:-ptxyz123}@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION:-true}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES:-false}
      - AIRFLOW__API__AUTH_BACKENDS=${AIRFLOW__API__AUTH_BACKENDS:-airflow.api.auth.backend.basic_auth}
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME:-admin}
      - _AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD:-admin}
      - MSSQL_SA_PASSWORD=${MSSQL_SA_PASSWORD:-PTXYZSecure123!}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      sqlserver:
        condition: service_healthy
    networks:
      - dw_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # Airflow Scheduler
  airflow-scheduler:
    build: 
      context: ./airflow
      dockerfile: Dockerfile
    container_name: ptxyz_airflow_scheduler
    command: scheduler
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID:-50000}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR:-CeleryExecutor}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:-postgresql+psycopg2://airflow:airflow@postgres/airflow}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD:-ptxyz123}@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION:-true}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES:-false}
      - AIRFLOW__API__AUTH_BACKENDS=${AIRFLOW__API__AUTH_BACKENDS:-airflow.api.auth.backend.basic_auth}
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
      - MSSQL_SA_PASSWORD=${MSSQL_SA_PASSWORD:-PTXYZSecure123!}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      sqlserver:
        condition: service_healthy
    networks:
      - dw_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # Airflow Worker (for Celery Executor)
  airflow-worker:
    build: 
      context: ./airflow
      dockerfile: Dockerfile
    container_name: ptxyz_airflow_worker
    command: celery worker
    environment:
      - AIRFLOW_UID=${AIRFLOW_UID:-50000}
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:-postgresql+psycopg2://airflow:airflow@postgres/airflow}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD:-ptxyz123}@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION:-true}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES:-false}
      - AIRFLOW__API__AUTH_BACKENDS=${AIRFLOW__API__AUTH_BACKENDS:-airflow.api.auth.backend.basic_auth}
      - MSSQL_SA_PASSWORD=${MSSQL_SA_PASSWORD:-PTXYZSecure123!}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      sqlserver:
        condition: service_healthy
    networks:
      - dw_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # Apache Superset for Data Visualization
  superset:
    image: apache/superset:latest
    container_name: ptxyz_superset
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_SECRET_KEY=${SUPERSET_SECRET_KEY}
    volumes:
      - superset_data:/app/superset_home
      - ./configs/superset:/app/superset_config
    networks:
      - dw_network
    depends_on:
      sqlserver:
        condition: service_healthy
    command: >
      bash -c "
      superset db upgrade &&
      superset fab create-admin --username ${SUPERSET_USERNAME} --firstname Admin --lastname User --email ${SUPERSET_EMAIL} --password ${SUPERSET_PASSWORD} &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger
      "

  # Jupyter Notebook for Data Analysis
  jupyter:
    image: jupyter/datascience-notebook:latest
    container_name: ptxyz_jupyter
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=${JUPYTER_ENABLE_LAB}
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/home/jovyan/work/data
    networks:
      - dw_network
    command: start-notebook.sh --NotebookApp.token='${JUPYTER_TOKEN}' --NotebookApp.password=''

  # Grafana for Dashboard and Monitoring
  grafana:
    image: grafana/grafana:latest
    container_name: ptxyz_grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=${GF_USERS_ALLOW_SIGN_UP}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - dw_network
    depends_on:
      sqlserver:
        condition: service_healthy

  # Metabase as alternative BI tool
  metabase:
    image: metabase/metabase:latest
    container_name: ptxyz_metabase
    ports:
      - "3001:3000"
    environment:
      - MB_DB_TYPE=${MB_DB_TYPE}
      - MB_DB_DBNAME=${MB_DB_DBNAME}
      - MB_DB_PORT=${MB_DB_PORT}
      - MB_DB_USER=${MB_DB_USER}
      - MB_DB_PASS=${MB_DB_PASS}
      - MB_DB_HOST=${MB_DB_HOST}
    volumes:
      - metabase_data:/metabase-data
    networks:
      - dw_network
    depends_on:
      postgres_metabase:
        condition: service_healthy

  # PostgreSQL for Metabase
  postgres_metabase:
    image: postgres:13
    container_name: ptxyz_postgres_metabase
    environment:
      - POSTGRES_DB=metabase
      - POSTGRES_USER=metabase
      - POSTGRES_PASSWORD=metabase
    volumes:
      - postgres_metabase_data:/var/lib/postgresql/data
    networks:
      - dw_network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "metabase"]
      interval: 5s
      retries: 5

  # Database Initialization Service
  db_init:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: ptxyz_db_init
    environment:
      - SA_PASSWORD=${MSSQL_SA_PASSWORD:-PTXYZSecure123!}
      - MSSQL_SA_PASSWORD=${MSSQL_SA_PASSWORD:-PTXYZSecure123!}
    volumes:
      - ./init-scripts:/docker-entrypoint-initdb.d
      - ./misi3:/scripts
    networks:
      - dw_network
    depends_on:
      sqlserver:
        condition: service_healthy
    command: /docker-entrypoint-initdb.d/init-db.sh
    restart: "no"

volumes:
  sqlserver_data:
  postgres_data:
  postgres_metabase_data:
  superset_data:
  grafana_data:
  metabase_data:

networks:
  dw_network:
    driver: bridge
